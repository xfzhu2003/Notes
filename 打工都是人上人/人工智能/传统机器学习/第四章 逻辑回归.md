![[逻辑回归/Cover.jpg]]
<!-- more -->

## 一、前言

`逻辑回归（Logistic Regression）`是一种经典的分类算法。虽然它的名字叫做逻辑回归，但它是一个分类问题，所以严格的名称应该叫做“逻辑分类”算法。

分类算法可以分为：
- 二分类
- 多分类

二分类就是将预测结果映射到两个值：0和1。多分类就是将结果映射到n个值：0、1、2、......

首先在这里我们先分析一个问题，为什么现行回归问题没有办法来解决分类问题，我们以二分来来举例。

我们给出如下数据集：{[0,0],[0.5,0],[1,0],[1.5,0],[2.5,1],[3,1],[3.5,1],[4,1]}

规定阈值为y=0.5。即当y>=0.5时，y=1。否则y=0。

根据数据集我们画出图像并拟合出如下直线：

![[逻辑回归/线性回归_1.jpg]]

在当前的拟合直线中，当y=0.5时x=2。也就是说：

- x<2时，  y=0
- x>=2时，y=1

针对于上述数据集，可以发现拟合效果似乎不错，并没有出现拟合错误的情况：即y值为0的训练实例x的小于2，y值为1的训练实例x都大于等于2。

下面我们对数据集做一下变动，加入一个训练实例：[6,1]

重新拟合我们的假设函数，图像如下：

![[逻辑回归/线性回归_2.jpg]]

我们发现，在当前的拟合直线中，当y=0.5时x=2.85。也就是说：

- x<2.85时，  y=0
- x>=2.85时，y=1

这时我们可以发现一个很严重的问题，就是对于训练实例[2.5,1]来说，如果按照上述拟合函数，则存在错误。

如果我们将加入的训练实例改为[10,1]呢？大家可以想象一下。

经过以上的说明，相信大家已经可以了解，线性回归无法应用于分类问题。当然具体的原因我并没有给出，只是利用直观的感受来理解这个问题，实际上这个问题有着很严密的数学解释，是关于概率论的相关知识，在这里也就不做过多的展开了。

## 二、二分类逻辑回归

### 2.1 假设函数

经过上述说明，我们了解到无法利用线性回归来解决二分类问题。那么究竟怎样的函数才能充分的拟合我们的数据呢。

关于假设函数的选择的过程在这里就不详述，当我们的知识积累到一定程度就可以理解。作为一个基础算法的学习笔记，我们这里直接给出假设函数，并向你证明这个假设函数的合理性。

在线性回归中，我们的假设函数的形式是：
$$
\begin{aligned}
& h_{\theta}(x)=\vec{\theta}^{\top}\vec{x} \\
& \vec{\theta}=[\theta_0,\theta_1,...,\theta_n]^{\top}\\
& \vec{x}=[x_0,x_1,...,x_n] \qquad x_0=1
\end{aligned}
$$



通过上述描述，我们知道这个假设函数是无法很好的拟合数据的，所以我们在此函数上进行一些更改。

我们改正的思路是，将线性规划中的假设函数的取值映射到0与1之间，通过设定阈值：高于阈值的y为1；低于阈值的y为0。以此来达到分类的目的。

我们引入SIGMOD函数（逻辑函数）来完成这个映射：
$$
g(z)=\frac{1}{1+e^{-z}}
$$

函数的图像如下：
![[逻辑回归/sigmoid.png]]

它的值域是：0<g(x)<1，将所有x映射到了0到1中间。所以我们利用他来改造我们的假设函数。



逻辑回归的假设函数如下：
$$
\begin{aligned}
& h_{\theta}(x)=g(\vec{\theta}^{\top}\vec{x}) \\
& g(z)=\frac{1}{1+e^{-z}} \\
& \vec{\theta}=[\theta_0,\theta_1,...,\theta_n]^{\top}\\
& \vec{x}=[x_0,x_1,...,x_n] \qquad x_0=1 \\
\end{aligned}
$$

将公式整合，得到的结果如下：
$$
\begin{aligned}
& h_{\theta}(x)=\frac{1}{1+e^{-(\vec{\theta}^{\top}\vec{x})}} \\
& \vec{\theta}=[\theta_0,\theta_1,...,\theta_n]^{\top}\\
& \vec{x}=[x_0,x_1,...,x_n] \qquad x_0=1 \\
\end{aligned}
$$

接下来我将向你展示这个假设函数的合理性：

考虑我们之前的那个例子，给定数据集如下：{[0,0],[0.5,0],[1,0],[1.5,0],[2.5,1],[3,1],[3.5,1],[4,1]}

我们利用最新的假设函数来拟合数据，通过计算我们得到了最优的参数列表：$[\theta_0,\theta_1]=[-20,10]$，那么我们的假设函数如下：

$$
h_{\theta}(x)=\frac{1}{1+e^{-(-20+10x)}}
$$

它的图像是：

![[逻辑回归/假设函数.jpg]]

我们观察可知，

- 当x的值在1.5至2.5之间时，会发生一家较大的变化。
- 当x的值小于1.5，y值十分接近0。
- 当x的值大于2.5，y值十分接近1。

我们发现数据被很好的拟合在了图像当中。

此外如果我们增加新的训练实例：[5,1]，那么对结果造成的影响将会微乎其微，因为[5,1]这个实例在当前假设函数中几乎被拟合在了这条曲线中，所以即使在训练集中加入它并重新计算得到参数，那么变化也是微乎其微的。

以上就是对新的假设函数的合理性的具体解释。

### 2.2 决策边界

下面我们来介绍另外一个概念，叫做`决策边界（Decision Boundary）`。

决策边界通过可视化的形式可以更好的帮助我们来理解分类问题。

首先，根据SIGMOD函数我们可知，当我们将阈值设置为0.5时：
- $\vec{\theta}^{\top}\vec{x}$>=0，y=1。
- $\vec{\theta}^{\top}\vec{x}$<0，  y=0。

假设我们特征有两个：$[x_1,x_2]$。

那么上述关系可以转化为：
- $\theta_0+\theta_0x_1+\theta_2x_2$>=0，y=1。
- $\theta_0+\theta_0x_1+\theta_2x_2$>=0，y=1。

根据以上关系，我们发现，如果画出$x_1$-$x_2$的函数图像，那么有一条线将所有数据一分为二，这条线就是：$\theta_0+\theta_0x_1+\theta_2x_2$=0。这也就是决策边界。

假设我们现在有一个已经拟合好的模型：$h_{\theta}(x)=g(\theta_0+\theta_1x_1+\theta_2x_2)$。其中：$[\theta_0,\theta_1,\theta_2]=[-3,1,1]$。

那么我们得到的决策边界为：$-3+x_1+x_2=0$。数据集和决策边界的图像如下：
![[逻辑回归/决策边界.jpg]]

我们可以看到，决策边界很好的将数据集进行了划分。

关于决策边界，有很重要一点需要特别说明。决策边界并不是数据集有的属性，而是我们的假设函数和参数$\vec{\theta}$具有的属性，一旦参数$\vec{\theta}$确定，决策边界也就确定了。

关于上述一段话的理解，你可以通俗的理解为：我们并不能根据数据集的形状来推测决策边界和假设函数。决策边界更多的是用来可视化的检查你的拟合是否优秀。

当然，通过构造复杂的多项式参数，我们可以得到其他形状的决策边界，在这里就不过多的展开了，大家可以深入的思考一下。

### 2.3 代价函数

在线性回归中，我们使用平方差函数来作为代价函数，但平方差函数并不适合。

从优化的角度来看，平方差函数对于逻辑回归来说是一个非凸函数，而非凸函数的优化往往会很困难，由于它存在很多局部最优解，我的梯度下降算法在运行过程中会受到很多因素的影响，初始值的选择就是其中一个，这是我们不希望看到的。

对于代价函数的优化问题，存在着专门谈论优化的学科，叫做“最优化理论”，其中有很多详细的概念：凸函数、非凸函数、凸优化等等。如果大家想要深入了解这方面知识，笔者推荐大家去阅读一下“凸优化”这本书。英文名字是“ConvexOptimization”。这本书有翻译版，在B站上也有中科大老师的课程录播。

所以，我们现在需要选择一个凸函数，并且他可以作为代价函数来评估当前参数对数据的拟合程度。下面直接给出逻辑回归的代价函数：

$$
\begin{align*}
& J(\theta) = \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)}) \\
& \mathrm{Cost}(h_\theta(x),y) = -\log(h_\theta(x)) \; & \text{if y = 1}\\
& \mathrm{Cost}(h_\theta(x),y) = -\log(1-h_\theta(x)) \; & \text{if y = 0}
\end{align*}
$$

当y=1时，$\mathrm{Cost}$函数图像如下：
![[逻辑回归/代价函数_1.png]]
我们发现，当$h_{\theta}(x)$越接近1时，Cost函数的值越小。反之则越大。

当y=0时，$\mathrm{Cost}$函数图像如下：
![[逻辑回归/代价函数_2-2486555.png]]
我们发现，当$h_{\theta}(x)$越接近0时，Cost函数的值越小。反之则越大。

也就是说，当$h_{\theta}(x)$与y越接近时，Cost函数的值越小，那么$J(\theta)$函数的值也就越小。所以$J(\theta)$函数可以用来评估当前参数对数据的拟合程度，同时它也是一个凸函数。

我们将Cost函数的形式进行一下变换：
$$
\mathrm{Cost}(h_\theta(x),y) = - y \; \log(h_\theta(x)) - (1 - y) \log(1 - h_\theta(x))
$$
这里通俗的解释一下变换的方法：

我们试着将y=1带入上式，可发现：
$$
\mathrm{Cost}(h_\theta(x),y) = -\log(h_\theta(x))
$$
我们试着将y=0带入上式，可发现：
$$
\mathrm{Cost}(h_\theta(x),y) = -\log(1-h_\theta(x))
$$
这就是两个公式组合的原理。

那么我们的代价函数的最终形式为：
$$
J(\theta)=-\frac{1}{m}(y^{(i)} \; \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)})))
$$

利用矩阵的计算方法为：
$$
\begin{aligned}
& \vec{h}=g(X\vec{\theta}) \\
& J(\theta)=\frac{1}{m}(-\vec{y}^{\top} \; \log(\vec{h}) - (1 - \vec{y}^{\top}) \log(1 - \vec{h}))
\end{aligned}
$$

需要注意的是，以上所有定义及解释，都是直接给出，并告诉你如何理解它，但其实都不是它真正的由来，逻辑回归代价函数是概率最大似然估计得来的，具体的方法大家可以单独去了解，在这里就不过多的具体的阐述了。

至此，代价函数的描述就结束了。

### 2.4 梯度下降算法

至此，梯度下降算法和之前的梯度下降算法就没有什么区别了：
$$
\begin{aligned}
& Repeat \; \lbrace \\
& \; \theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j}J(\theta) \\
& \rbrace
\end{aligned}
$$

求得导数并代入公式：
$$
\begin{align*} 
& Repeat \; \lbrace \\
& \; \theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\
& \rbrace 
\end{align*}
$$

可以发现，上面的梯度下降的最终形式与线性回归中的梯度下降形式相同。但要注意，因为假设函数不同，所以其实二者并不相同。

梯度下降的矩阵计算形式如下：
$$
\vec{\theta} := \vec{\theta} - \frac{\alpha}{m} X^{\top}(g(X\vec{\theta})-\vec{y})
$$
至此，关于二分类逻辑回归的问题就已经描述完了。



##  三、多分类逻辑回归

多分类线性回归多会将结果映射到n个值。实现多分类逻辑回归的方式有很多种：
- One-vs-One（OvO）
- One-vs-Rest（OvR）
- Softmax

在这里我们简单介绍一下前两种方式。

### 3.1 OvO
假设现在训练数据集的分布如下图所示（其中A，B，C代表训练数据的类别）：

![[逻辑回归/OvO_1.jpg]]

如果想要使用逻辑回归算法来解决这种3分类问题，可以使用OvO。OvO(One Vs One)是使用二分类算法来解决多分类问题的一种策略。从字面意思可以看出它的核心思想就是一对一。所谓的“一”，指的是类别。而“对”指的是从训练集中划分不同的两个类别的组合来训练出多个分类器。

划分的规则很简单，就是组合（$C_n^2$，其中n表示训练集中类别的数量，在这个例子中为3）。如下图所示（其中每一个矩形框代表一种划分）：

![[逻辑回归/OvO_2.png]]

分别用这3种划分，划分出来的训练集训练二分类分类器，就能得到3个分类器。此时训练阶段已经完毕。如下图所示：

![[逻辑回归/OvO3.jpg]]

在预测阶段，只需要将测试样本分别扔给训练阶段训练好的3个分类器进行预测，最后将3个分类器预测出的结果进行投票统计，票数最高的结果为预测结果。如下图所示：

![[逻辑回归/OvO_4.jpg]]

### 3.2 OvR

假设现在训练数据集的分布如下图所示（其中A，B，C代表训练数据的类别）：

![[逻辑回归/OvR_1.jpg]]

如果想要使用逻辑回归算法来解决这种3分类问题，可以使用OvR。OvR(One Vs Rest)是使用二分类算法来解决多分类问题的一种策略。从字面意思可以看出它的核心思想就是一对剩余。一对剩余的意思是当要对n种类别的样本进行分类时，分别取一种样本作为一类，将剩余的所有类型的样本看做另一类，这样就形成了n个二分类问题。所以和OvO一样，在训练阶段需要进行划分。

划分也很简单，如下图所示：

![[逻辑回归/OvR_2.jpg]]

分别用这3种划分，划分出来的训练集训练二分类分类器，就能得到3个分类器。此时训练阶段已经完毕。如下图所示：

![[逻辑回归/OvR_3.png]]

在预测阶段，只需要将测试样本分别扔给训练阶段训练好的3个分类器进行预测，最后选概率最高的类别作为最终结果。如下图所示：


![[逻辑回归/OvR_4.jpg]]

当然还有Softmax函数的映射方式，但是涉及到很复杂的概率知识，在这里就不过多展开了，等到深度学习算法时再详细描述这些。



## 四、高级优化算法

我们在梯度下降算法了解到，我们利用$\theta$参数的偏导（梯度）来进行迭代计算。如果要监测代价函数是否收敛，我们还需计算代价函数的值。

在实际的应用过程中，我们有许多优秀的算法可以代替梯度下降来进行对$\theta$参数的迭代，它们需要的参数与梯度下降相同（内部实现与梯度下降不同）。所以可以理解为高级的梯度下降，以下算法就拥有这种功能：

- 共轭梯度
- BFGS
- L-BFGS

在这里我们不去讨论它们的内部实现，有兴趣的读者可以自己去了解一下。我们在这里只强调一点，有很多语言以及很多工具包已经为我们提供了这类函数的实现，我们只需调用它们提供的API就好。如果你对这些算法背后的数学推导过程无法深入理解，那么一定不要试着手动去实现这些算法。