![封面](多变量线性回归/Cover.jpg)
<!-- more -->

## 一、前言

多变量线性回归问题和单变量线性回归问题相似，他们的定义、描述都极其相似，解决方法也通用。我们可以采用平方差代价函数+梯度下降优化算法来确定假设函数并拟合数据。不过在本节中，关于平方差代价函数，我们使用另一种优化算法来进行优化——`正规方程`。

## 二、模型和代价函数

### 2.1 模型表示
为了建立供将来使用的符号，定义以下概念：
- $x^{(i)}$表示“输入”变量,也称为输入要素。
- $x_{j}$表示第j个特征（因变量）。
- m表示训练集中训练实例的个数。
- n表示特征的个数（因变量）。

注意：

- 上标“i”只是训练集中的一个索引，与幂运算无关，取值范围：1~m。
- 下标“j”表示的第j个特征，取值范围：1~n。

假设函数为：$h_\theta (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \cdots + \theta_n x_n$

### 2.2 代价函数
多变量线性回归中我们使用的代价函数依旧是平方差代价函数，有关代价函数和平方差代价函数的详细介绍可以查看上一篇文章。

在多变量线性回归中，平方差代价函数定义如下：
$$
J(\theta_{0},\theta_{1},...,\theta_{n})=\frac{1}{2m}\sum_{i=1}^m (h_{ \theta}(x^{(i)})-y^{(i)})^{2}
$$

将$h_{\theta }(x)$带入则可得到如下函数：

$$
J(\theta_{0},\theta_{1},...,\theta_{n}) = \frac{1}{2m}\sum_{i=1}^m （\theta_0 + \theta_1 x_1^{(i)} + \theta_2 x_2^{(i)} + \theta_3 x_3^{(i)} + \cdots + \theta_n x_n^{(i)} - y^{(i)}）^{2}
$$

## 三、参数学习

### 3.1 梯度下降

#### 3.1.1 梯度下降的定义

多变量线性回归中梯度下降的使用与单变量线性回归相同，关于梯度下降的详细介绍可以查看上一篇文章。
在多变量线性回归中，梯度下降定义如下：
$$
\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1,... ,\theta_n) \qquad j=0,1,...n
$$
注意，在每次迭代$\theta _{j}$时，应同时更新参数 $\theta _{0}$，$\theta _{1}$，...，$\theta _{n}$。

我们将$h_\theta (x)$带入迭代公式可以得到如下形式：
$$
\begin{aligned}  \text{repeat until convergence:} &\lbrace \\  \theta_0 :=& \theta_0 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \\  \theta_1 :=& \theta_1 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_1^{(i)} \\  \theta_2 :=& \theta_2 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_2^{(i)} \\ &\cdots \\ \rbrace \end{aligned}
$$

#### 3.1.2 关于$\alpha$的进一步讨论
在单变量线性回归问题中，我们简单的分析了$\alpha$，也就是学习速率这个因素。我们提到：

- 当$\alpha$过小时，到达最优解的步数多，时间长。当数据集大时特别明显。
- 当$\alpha$过大时，有可能跳过最优解，从而导致无法收敛（无法到达最优解）。

那么到底要怎么选择$\alpha$呢？这里笔者只给出个人的一些建议，其实选择合适的$\alpha$也是我们调节参数的一个很重要的步骤。
$$
{-0.03，-0.01，-0.3，-0.1，0.01，0.03，0.1，0.3}
$$
这个是比较推荐的一些选择，一般默认选择0.01作为$\alpha$的初始值，然后根据它在模型中表现出来的特点再进行进一步的、更精确的调整。

#### 3.1.3 梯度下降的计算方式
在计算时我们依旧存在两种计算方法：

- 迭代法
- 矩阵法

关于迭代法就不做过多介绍，需要注意的是：迭代法与矩阵法，是在固定迭代次数的情况下计算训练集的平方差时使用的方法（迭代法是一个一个训练实例的计算，矩阵法是所有训练实例一起计算），梯度下降中的迭代还是要进行的。希望大家不要混淆。

下面主要介绍矩阵法。
我们将上述迭代公式做一下改变：
$$
\begin{align*} & \text{repeat until convergence:} \; \lbrace \newline \; & \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_0^{(i)}\newline \; & \theta_1 := \theta_1 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_1^{(i)} \newline \; & \theta_2 := \theta_2 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_2^{(i)} \newline & \cdots \newline \rbrace \end{align*}
$$

针对于上述改变，我们设：$x_0^{(i)}=1\qquad i=0,1,...,m$
也就是说针对于训练集中的每个训练实例，我们在第一位插入一个1来表示第“0”个特征，它是我们自定义的特征（准确来讲它并不是一个特征，因为所有训练实例它都是1，不存在差异），只是用来方便计算而已。

针对于训练集，我们将其拆分为两部分：

- 将训练集中的所有$y^{i}$构成一个$m\times 1$的矩阵（向量）
$$
\vec{y}=[y^{1},y^{2},...\quad,y^{m}]^\top
$$
- 将训练集中的所有$x_j^{(i)}$构成一个$m\times (n+1)$的矩阵（矩阵的第一列用1填充，第二列为$x^{i}$的集合）：
$$
X=\begin{bmatrix}
1 & x_1^{(1)} & ... & x_n^{(1)}\\ 
 1&x_1^{(2)} & ... & x_n^{(2)}\\ 
 ...& ...& ... & ...\\ 
 1& x_1^{(m)} & ... & x_n^{(m)}
\end{bmatrix}
$$
- 将所有参数$\theta$构成一个$(n+1)\times 1$的矩阵：
$$
\vec{\theta}=[\theta_0,\theta_1,...,\theta_n]^\top
$$

根据以上定义，我们可以一次性计算所有训练实例： 

将特征构建成一个特征向量：
$$
\vec{x}=[x_0,x_1,x_2,...,x_n]^\top
$$
规定 $x_0=1$。

则假设函数可以写作如下形式：
$$
h_{\theta }(x)=\vec{\theta}^{\top}\vec{x}
$$
则所有由$h_{\theta }(x)$函数计算出的预测结果构成的向量为：
$$
\vec{y`}=X\vec{\theta}
$$

代价函数可以利用下面的公式计算，其中$\sum$符号的含义是计算矩阵中（其实是一个$m\times 1$的向量）所有元素的和。
$$
J(\theta_{0},\theta_{1})=\frac{1}{2m}\sum(\vec{y`} - \vec{y})^{2}\\
J(\theta_{0},\theta_{1})=\frac{1}{2m}\sum(X\vec{\theta} - \vec{y})^{2}\\
J(\theta_{0},\theta_{1})=\frac{1}{2m}(X\vec{\theta} - \vec{y})^\top(X\vec{\theta} - \vec{y})
$$

那么梯度下降的公式即可化为：
$$
\vec{\theta}=\vec{\theta}-\alpha \frac{1}{m}((X\vec{\theta} - \vec{y})^\top X)^\top
$$
将转置带入并展开可以得到：
$$
\vec{\theta}=\vec{\theta}-\alpha \frac{1}{m}*(X^\top(X\vec{\theta} - \vec{y}))
$$

最后会得到一个$(n+1)\times 1$的$\vec{\theta}$向量，即为更新过后的$\theta$参数的组成的向量。

我们迭代上述过程，直到$\theta$参数收敛。

*可以发现，除了X矩阵和$\vec{\theta}$向量外，其余公式基本没有变化与、单变量线性回归相同。这就是矩阵法的优势——除了训练集数据处理外，几乎所有代码都可以通用，而迭代法则需要重新编写更新$\theta$参数时的代码。*

#### 3.1.4 特征缩放
特征缩放只针对于多变量线性回归，当特征存在多个时，可能各个特征之间的取值范围会有很大的差异。比如针对于$x_1$的取值范围是-1到1，但$x_2$的取值范围是3000-10000。这会导致我们的收敛速度很慢，收敛需要的迭代次数会特别大。这是因为$\theta$在小范围内会迅速下降，而在大范围内会缓慢下降。

为了解决这个问题，我们可以通过将每个输入值都设置在大致相同的范围内来加快梯度下降的速度。防止这种情况的方法是修改输入变量的范围，以使它们都大致相同。理想情况下：
$$
−1 ≤ x_{(i)}≤ 1
$$
或者是
$$
−0.5 ≤ x_{(i)}≤ 0.5
$$

但这些不是确切的要求，我们只是试图加快速度。目标是使所有输入变量大致进入这些范围之一，只要保证他们不要相差太多即可。

有助于此的两种技术是`特征缩放`和`均值归一化`。
- 特征缩放涉及将输入值除以输入变量的范围（即最大值减去最小值），从而得到的新范围仅为1。
- 均归一化涉及从输入变量的值中减去输入变量的平均值。输入变量导致输入变量的新平均值仅为零。

或许你对以上说法还是一头雾水，不过要实现这两种技术，只要按照以下公式调整输入值就可以了：
$$
x_{i}=\frac{x_{i}-\mu_{i}}{s_{i}}
$$
其中：
- $\mu_{i}$表示为$x_{i}$特征所有输入的平均值。
- $s_{i}$表示为$x_{i}$特征所有输入的标准差。

经过以上变换后，基本所有的特征都会变成在一个相似的取值范围内。

*请注意，当你对某个特征变量做过特征缩放并训练出模型后、在应用模型预测问题时，请记得一定要按相同的方式处理输入数据，不然你可能得到一个奇怪的结果。*

### 3.2 正规方程

#### 3.2.1 正规方程介绍
`正规方程（Normal Equation）`，也叫正态方程。是我们优化代价函数的区别于梯度下降的第二种方式。

想必大家已经清楚，我们优化代价函数的目的，就是寻找代价函数最小值是所对应的各个参数的值。根据以往学过的知识我们可知，当导数为0是，函数取得极值，而根据我们在单变量线性回归中了解到的平方差函数的图像可知。我们获得的极值往往是整个函数的最小值。当然，即使我这么描述你可能依旧无法理解，你可能会问，那也许是极大值呢？在此我们不再对这个问题做太过深入的讨论，不过关于这个问题已经有严谨的数学证明，想了解的同学可以额外的去了解。

让我们借用上面的定义，我们最终要求解的$\vec{\theta}$的公式为：
$$
\vec{\theta}=(X^{\top}X)^{-1} X^{\top}\vec{y}
$$

#### 3.2.2 正规方程的推导
这一小节主要是正规方程的推导过程，不感兴趣的同学可以直接跳到下一节中。

在开始推导之前，我先给出几个公式。这些公式都是被证明过的，感兴趣的同学可以自行了解证明过程，这里就直接使用了。
$$
\begin{aligned}
1. &\frac{\partial X^{\top} A X}{\partial X}=(A+A^{\top})X\\
2. &\frac{\partial X^{\top} A}{\partial X}=A\\
3. &\frac{\partial A X}{\partial X}=A^{\top}\\
4. &(A\pm B)^{\top}=A^{\top}+B^{\top}\\
5. &(AB)^{\top}=B^{\top}A^{\top}\\
6. &A^{-1}A=AA^{-1}=I
\end{aligned}
$$

基于以上公式，我们开始推导。

观察代价函数:
$$
J(\theta_{0},\theta_{1},...,\theta_{n})=\frac{1}{2m}\sum_{i=1}^m (h_{ \theta}(x^{(i)})-y^{(i)})^{2}\\
$$
我们发现,求取$J(\theta_{0},\theta_{1},...,\theta_{n})$的最小值，就是求取后面那个求和符号的最小值。我们设函数：
$$
Q(\theta_{0},\theta_{1},...,\theta_{n})=\sum_{i=1}^m (h_{ \theta}(x^{(i)})-y^{(i)})^{2}
$$
那么接下来的问题就变成了求取$Q(\theta_{0},\theta_{1},...,\theta_{n})$的最小值。

我们将$Q(\theta_{0},\theta_{1},...,\theta_{n})$改写为如下形式：
$$
Q(\theta_{0},\theta_{1},...,\theta_{n})=((X \vec{\theta})-\vec{y})^{\top}((X \vec{\theta})-\vec{y})
$$
根据正规方程的描述，我们只需让其关于$\vec{\theta}$的导数为0即可。

推导过程如下：
$$
\begin{aligned}
&((X\vec{\theta }-\vec{y})^{\top}(X\vec{\theta }-\vec{y}))^{`}=0\\
&\Longrightarrow ((\vec{\theta }^{\top}X-\vec{y}^{\top})(X\vec{\theta }-\vec{y}))^{`}=0\\
&\Longrightarrow ((\vec{\theta }^{\top}X^{\top}X\vec{\theta })-(\vec{\theta }^{\top}X^{\top}\vec{y})-(\vec{y}^{\top}X\vec{\theta })+(\vec{y}^{\top}\vec{y}))^{`}=0\\
&\Longrightarrow (X^{\top}X+X^{\top}X)\vec{\theta }-X^{\top}\vec{y}-X^{\top}\vec{y}+0=0\\
&\Longrightarrow 2X^{\top}X\vec{\theta }-2X^{\top}\vec{y}=0\\
&\Longrightarrow X^{\top}X\vec{\theta }=X^{\top}\vec{y}\\
&\Longrightarrow \vec{\theta }=(X^{\top}X)^{-1} X^{\top}\vec{y}\\
\end{aligned}
$$
结合上面提供的公式，相信大家很快就能推理出来。

#### 3.2.3 正规方程中的注意事项

对于我们求解$\vec{\theta}$的公式：
$$
\vec{\theta }=(X^{\top}X)^{-1} X^{\top}\vec{y}
$$
大家需要注意的是，在方程中存在这样一项：
$$
(X^{\top}X)^{-1}
$$

对于矩阵来说，并不是所有矩阵都是可逆的，其中存在一些不可逆的矩阵，我们称其为`奇异矩阵`。

同样，我们在运行程序时可能也会出现这种情况。关于这种情况，在此我们不做过多的数学方面的讨论，只给出结论，感兴趣的同学可以去了解具体的证明。

如果是$(X^{\top}X)^{-1}$不可逆的，常见原因可能有：

- 冗余特征，其中两个特征密切相关（即，它们线性相关）
- 特征太多（例如m≤n）。在这种情况下，删除某些功能或使用“正则化”。

解决上述问题的方法包括删除与另一个线性相关的要素，或者在要素过多时删除一个或多个要素。

### 3.3 梯度下降和正规方程
既然存在两种优化算法，那么当然就存在着优缺点的问题。

以下是梯度下降与正规方程的比较：

| 梯度梯度                 | 正规方程                |
| :----------------------- | :---------------------- |
| 需要选择Alpha            | 无需选择Alpha           |
| 需要多次迭代             | 无需迭代                |
| $\mathcal {O}（kn ^ 2）$ | $\mathcal {O}（n ^ 3）$ |
| 当n大时效果很好          | 如果n很大，则变慢       |

利用正规方程计算逆矩阵的复杂度为$\mathcal {O}（n ^ 3）$。因此，如果我们具有大量特征，则正规方程将很慢。

在实践中，当n超过10,000时，可能是从正规解转到迭代过程的好时机。

## 四、多项式回归与多变量线性回归

如果我们的假设函数不能很好地拟合数据，那么我们可以考虑使用非线性的假设函数来拟合数据。我们可以通过将特征设为二次，三次、平方根、指数或对数等等形式来更改假设函数的曲线。

例如，如果我们的假设函数是$h_\theta(x) = \theta_0 + \theta_1 x_1$，但是我们发现拟合效果可能不好。我们可以基于 $x_1$ ，得到二次函数$h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_1^2$ 或三次函数$h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_1^2+\theta_3 x_1^3$。

在三次函数中，我们引入了新的特征：$x_2=x_1^2$和$x_3=x_1^3$。

所以三次函数我们可以写作：
$$
h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2+\theta_3 x_3
$$

这样就从一个单变量多项式回归问题转换成了一个多变量线性回归问题。这样就可以利用我们现有的知识解决了。

要记住的一件事是，如果以这种方式选择特征，则特征缩放就变得非常重要。

例如：如果$x_1$的范围是1-1000，则$x_2=x_1 ^ 2$的范围就变成了1-1000000，而$ x_3=x_1 ^ 3$的范围就变成了1-1000000000。此时即使使用正规方程来解决问题，也可能因为数据类型的范围溢出引发异常，所以千万要记得进行“完整”的特征缩放。

关于这部分我们也不详细展开了，具体的更复杂的形式大家可以去详细了解。这是在这给大家提供一种解决问题的思路，有时很多的未知问题可以转化为已知问题来进行解决。

## 五、多变量线性回归实例